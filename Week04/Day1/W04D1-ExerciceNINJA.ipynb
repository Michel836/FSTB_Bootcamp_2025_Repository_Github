{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b516725",
   "metadata": {},
   "source": [
    "# Exercises XP Ninja\n",
    "done 2025-07-01\n",
    "\n",
    "What you will learn\n",
    "* how to train a logistic regression model\n",
    "* how to evaluate your model with metricq\n",
    "\n",
    "\n",
    "* What you will create\n",
    "a script to implement Python functions to calculate Accuracy, Precision, Recall, and F1-Score from scratch.\n",
    "\n",
    "\n",
    "## Exercise 1 : Evaluation Metrics Implementation\n",
    "### Objective:\n",
    "Implement Python functions to calculate Accuracy, Precision, Recall, and F1-Score from scratch.\n",
    "\n",
    "### Dataset:\n",
    "Use any binary classification dataset, for example, the Breast Cancer Wisconsin (Diagnostic) dataset available in scikit-learn or any other binary classification dataset of your choice.\n",
    "\n",
    "### Tasks:\n",
    "* Split the dataset into training and test sets.\n",
    "* Train a simple classification model (like Logistic Regression) on the training set.\n",
    "* Make predictions on the test set.\n",
    "* Write functions to calculate Accuracy, Precision, Recall, and F1-Score using the confusion matrix components (TP, TN, FP, FN).\n",
    "* Evaluate your model using these metrics and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763aa3dd",
   "metadata": {},
   "source": [
    "## 1 – Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24e7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b1463",
   "metadata": {},
   "source": [
    "## 2 – Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2010bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des données : (569, 30)\n",
      "Classes : [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Chargement du jeu de données Breast Cancer (classification binaire)\n",
    "data = load_breast_cancer()\n",
    "X = data.data       # variables explicatives\n",
    "y = data.target     # étiquettes (0 = maligne, 1 = bénigne)\n",
    "\n",
    "# Affichage rapide de la forme des données\n",
    "print(\"Forme des données :\", X.shape)\n",
    "print(\"Classes :\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5090e8",
   "metadata": {},
   "source": [
    "## 3 – Séparation en train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4041aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille train : 455\n",
      "Taille test : 114\n"
     ]
    }
   ],
   "source": [
    "# Division du jeu de données : 80% entraînement, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Taille train :\", X_train.shape[0])\n",
    "print(\"Taille test :\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7de2f",
   "metadata": {},
   "source": [
    "## 4 – Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e3109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création et entraînement d’un modèle de régression logistique\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur le jeu de test\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05412f0",
   "metadata": {},
   "source": [
    "## 5 – Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff3b327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vrai négatifs (TN): 39\n",
      "Faux positifs (FP): 4\n",
      "Faux négatifs (FN): 1\n",
      "Vrai positifs (TP): 70\n"
     ]
    }
   ],
   "source": [
    "# Création de la matrice de confusion : [TN, FP, FN, TP]\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(f\"Vrai négatifs (TN): {tn}\")\n",
    "print(f\"Faux positifs (FP): {fp}\")\n",
    "print(f\"Faux négatifs (FN): {fn}\")\n",
    "print(f\"Vrai positifs (TP): {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d622209",
   "metadata": {},
   "source": [
    "## 6 – Fonctions de calcul des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e03316ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul manuel des métriques d'évaluation\n",
    "\n",
    "def accuracy(tp, tn, fp, fn):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "def f1_score(prec, rec):\n",
    "    return 2 * (prec * rec) / (prec + rec) if (prec + rec) != 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e3323",
   "metadata": {},
   "source": [
    "## 7 – Évaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb401803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  : 0.9561\n",
      "Precision : 0.9459\n",
      "Recall    : 0.9859\n",
      "F1-Score  : 0.9655\n"
     ]
    }
   ],
   "source": [
    "# Application des fonctions sur les résultats\n",
    "acc = accuracy(tp, tn, fp, fn)\n",
    "prec = precision(tp, fp)\n",
    "rec = recall(tp, fn)\n",
    "f1 = f1_score(prec, rec)\n",
    "\n",
    "# Affichage\n",
    "print(f\"Accuracy  : {acc:.4f}\")\n",
    "print(f\"Precision : {prec:.4f}\")\n",
    "print(f\"Recall    : {rec:.4f}\")\n",
    "print(f\"F1-Score  : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2566b5",
   "metadata": {},
   "source": [
    "## Ces résultats montrent que :\n",
    "\n",
    "* **Accuracy (95.61 %)** : Le modèle est globalement très précis sur tous les cas.\n",
    "* **Precision (94.59 %)** : Parmi les cas prédits comme positifs (bénins), 94.59 % sont réellement corrects → peu de faux positifs.\n",
    "* **Recall (98.59 %)** : Le modèle capte presque tous les vrais cas positifs → très peu de faux négatifs.\n",
    "* **F1-Score (96.55 %)** : Excellent équilibre entre précision et rappel.\n",
    "\n",
    "### **Interprétation** : \n",
    "Ce modèle est **très fiable** pour un cas médical où il est crucial de **ne pas rater un cas réel (recall élevé)**, tout en **limitant les fausses alertes (bonne précision)**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
