{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ca4e15",
   "metadata": {},
   "source": [
    "# Exercises XP Gold\n",
    "Done 2025-07-01\n",
    "\n",
    "## What will you create\n",
    "calculation sheets for metrics like Accuracy, Precision, Recall, and F1-Score based on given confusion matrix values.\n",
    "develop strategic proposals that address specific challenges posed by these issues.\n",
    "build frameworks for evaluating classification models in different scenarios, especially focusing on the impact of class imbalance and the role of threshold tuning.\n",
    "\n",
    "\n",
    "## What will you learn\n",
    "Understand and interpret the confusion matrix in different scenarios.\n",
    "Understand the trade-offs between different evaluation metrics.\n",
    "Explore the concepts of cross-validation and learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2a4ec",
   "metadata": {},
   "source": [
    "## **Exercise 1 : Analyzing Confusion Matrix**\n",
    "* Instructions, Imagine you have a dataset for a binary classification problem, such as email spam detection, where emails are classified as either ‘Spam’ or ‘Not Spam’. You are provided with the confusion matrix results of a classifier.\n",
    "- Define in your own words what True Positives, True Negatives, False Positives, and False Negatives mean in the context of this email spam detection problem.\n",
    "- Given a confusion matrix with specific values for TP, TN, FP, FN, calculate the Accuracy, Precision, Recall, and F1-Score.\n",
    "- Discuss how the classifier’s performance would change with a higher number of False Positives compared to False Negatives, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae06dd",
   "metadata": {},
   "source": [
    "## **Analyse d’une matrice de confusion dans un problème de détection de spam** :\n",
    "\n",
    "---\n",
    "\n",
    "## **Définitions dans le contexte \"Spam / Pas Spam\"**\n",
    "\n",
    "* **True Positives (TP)** : Emails correctement identifiés comme **Spam**\n",
    "  * Exemple : un vrai spam détecté comme tel.\n",
    "\n",
    "* **True Negatives (TN)** : Emails correctement identifiés comme **Non Spam**\n",
    "  * Exemple : un email légitime qui n’est pas marqué spam.\n",
    "\n",
    "* **False Positives (FP)** : Emails **non spam** mal classés comme **Spam**\n",
    "  * Exemple : un email important mis en spam par erreur.\n",
    "\n",
    "* **False Negatives (FN)** : Emails **spam** mal classés comme **Non Spam**\n",
    "  * Exemple : un spam qui atterrit dans la boîte de réception.\n",
    "\n",
    "---\n",
    "\n",
    "## **Calcul des métriques** (avec valeurs fictives)\n",
    "\n",
    "**Exemple** :\n",
    "\n",
    "* TP = 80\n",
    "* TN = 90\n",
    "* FP = 20\n",
    "* FN = 10\n",
    "\n",
    "Formules :\n",
    "\n",
    "* **Accuracy** = (TP + TN) / (TP + TN + FP + FN)\n",
    "  \\= (80 + 90) / (80 + 90 + 20 + 10) = 170 / 200 = **0.85**\n",
    "\n",
    "* **Precision** = TP / (TP + FP)\n",
    "  \\= 80 / (80 + 20) = **0.80**\n",
    "\n",
    "* **Recall** = TP / (TP + FN)\n",
    "  \\= 80 / (80 + 10) = **0.89**\n",
    "\n",
    "* **F1-Score** = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "  \\= 2 × (0.80 × 0.89) / (0.80 + 0.89) ≈ **0.84**\n",
    "\n",
    "---\n",
    "\n",
    "## **Impact des erreurs FP vs FN**\n",
    "\n",
    "* **Trop de FP (faux positifs)** :\n",
    "  * Trop d’emails **légitimes** marqués comme spam\n",
    "  * Cela peut faire **perdre des messages importants** → mauvaise expérience utilisateur.\n",
    "\n",
    "* **Trop de FN (faux négatifs)** :\n",
    "  * Trop de **spams** non détectés\n",
    "  * Cela **pollue la boîte de réception**, mais moins grave si le filtre est globalement bon.\n",
    "\n",
    "## **Conclusion** :\n",
    "\n",
    "* Si l’enjeu est **ne pas rater des emails importants** → il faut **minimiser les FP** (favoriser la **précision**).\n",
    "* Si l’enjeu est **filtrer tous les spams à tout prix** → il faut **minimiser les FN** (favoriser le **rappel**).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd3d7e",
   "metadata": {},
   "source": [
    "## **Exercise 2 : Evaluating Trade-offs in Metrics**\n",
    "### Instructions\n",
    "Consider a medical diagnosis application where a model predicts whether patients have a certain disease.\n",
    "- Explain why high recall is more important than high precision in this medical diagnosis context.\n",
    "- Describe a scenario where precision becomes more important than recall.\n",
    "- Discuss the potential consequences of focusing solely on improving accuracy in imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b789f3",
   "metadata": {},
   "source": [
    "## **Évaluer les compromis entre les métriques dans un contexte médical** :\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Pourquoi le rappel (Recall) est plus important que la précision dans ce cas médical ?**\n",
    "\n",
    "* **Rappel élevé** = détecter **le plus grand nombre possible de malades**\n",
    "* Dans un contexte médical, **rater un patient malade (FN)** peut entraîner un **retard de diagnostic** ou **mettre sa vie en danger**.\n",
    "* Il vaut mieux **faux alarmer** un patient sain (FP) que **louper un vrai cas**.\n",
    "\n",
    "**Mieux vaut alerter à tort qu’ignorer un malade.**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Quand la précision devient-elle plus importante ?**\n",
    "\n",
    "**Exemple** : Une alerte automatique pour déclencher un traitement coûteux, invasif ou risqué (ex : chimiothérapie, chirurgie).\n",
    "\n",
    "* **Précision élevée** = on veut **éviter les faux positifs**\n",
    "* On veut être sûr que **les cas détectés sont réellement positifs** avant d’agir lourdement.\n",
    "\n",
    "**Mieux vaut ne rien faire que de faire du mal inutilement.**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Problèmes si on optimise uniquement l’Accuracy sur un jeu déséquilibré**\n",
    "\n",
    "* Si 99 % des patients sont sains, un modèle qui prédit \"sain\" pour tout le monde aura **99 % de précision**... mais il **rate tous les malades** !\n",
    "* L’accuracy **masque les erreurs critiques** dans des cas déséquilibrés.\n",
    "\n",
    "**Conséquence** : modèle apparemment performant, mais **inutilisable en pratique**.\n",
    "\n",
    "---\n",
    "\n",
    "Il est donc **essentiel de choisir la bonne métrique** selon le **contexte, le risque et les conséquences** des erreurs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5d03f",
   "metadata": {},
   "source": [
    "## **Exercise 3 : Understanding Cross-Validation and Learning Curves**\n",
    "### Instructions\n",
    "You are working on a project with a large dataset that involves predicting housing prices based on various features.\n",
    "- Explain the difference between K-Fold Cross-Validation and Stratified K-Fold Cross-Validation. Which one would you choose for this task and why?\n",
    "- Describe what learning curves are and how they can help in understanding the performance of your model.\n",
    "- Discuss the implications of underfitting and overfitting as observed from learning curves, and how you might address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd24473",
   "metadata": {},
   "source": [
    "## **Validation croisée et courbes d’apprentissage (projet de prédiction de prix immobiliers)** :\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Différence entre K-Fold et Stratified K-Fold**\n",
    "\n",
    "### **K-Fold Cross-Validation**\n",
    "\n",
    "* Divise les données en *k* sous-ensembles.\n",
    "* À chaque itération, un pli est utilisé pour tester, les autres pour entraîner.\n",
    "* Utilisé pour évaluer la stabilité d’un modèle.\n",
    "\n",
    "### **Stratified K-Fold Cross-Validation**\n",
    "\n",
    "* Même principe, **mais en conservant la proportion des classes** (utile pour classification).\n",
    "* Assure un bon équilibre entre classes dans chaque pli.\n",
    "\n",
    "**Choix ici** : **K-Fold classique**, car **la régression (prix)** ne nécessite pas de balance entre classes.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Courbes d’apprentissage (Learning Curves)**\n",
    "\n",
    "Les **courbes d’apprentissage** montrent l’évolution des performances du modèle en fonction de la taille de l’échantillon d’entraînement.\n",
    "\n",
    "* Deux courbes : **erreur d’entraînement** et **erreur de validation**\n",
    "* Permettent de diagnostiquer :\n",
    "\n",
    "  * Si le modèle apprend correctement\n",
    "  * Si plus de données améliorent la performance\n",
    "\n",
    "---\n",
    "\n",
    "##  **3. Underfitting vs Overfitting selon les courbes**\n",
    "\n",
    "| Situation        | Symptômes sur les courbes                      | Solutions possibles                                             |\n",
    "| ---------------- | ---------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **Underfitting** | Erreurs élevées sur train **et** validation    | ➤ Modèle trop simple<br>➤ Ajouter features ou complexité        |\n",
    "| **Overfitting**  | Erreur faible sur train, élevée sur validation | ➤ Modèle trop complexe<br>➤ Regularisation<br>➤ Plus de données |\n",
    "\n",
    "---\n",
    "\n",
    "Les courbes d’apprentissage aident à **ajuster la complexité du modèle** et **guider les améliorations**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65284a10",
   "metadata": {},
   "source": [
    "## **Exercise 4 : Impact of Class Imbalance on Model Evaluation**\n",
    "### Instructions\n",
    "Imagine you are working on a dataset for detecting a rare disease where only 2% of the instances are positive cases (have the disease).\n",
    "- Explain why using accuracy as an evaluation metric might be misleading in this scenario.\n",
    "- Discuss the importance of precision and recall in the context of this imbalanced dataset.\n",
    "- Propose strategies you could use to more effectively evaluate and improve the model’s performance in this scenario, considering the imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e5f59",
   "metadata": {},
   "source": [
    "## **Impact du déséquilibre des classes sur l’évaluation du modèle** (détection d’une maladie rare à 2 %) :\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Pourquoi l’accuracy est trompeuse ici**\n",
    "\n",
    "* Si 98 % des cas sont négatifs (sains), un modèle qui prédit \"négatif\" pour tous aura **98 % d’accuracy**…\n",
    "  **Mais il ne détecte aucun malade.**\n",
    "\n",
    "**Accuracy** donne donc une **illusion de performance** dans les jeux de données déséquilibrés.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Importance de la précision et du rappel**\n",
    "\n",
    "* **Recall (rappel)** : détecter un maximum de cas malades (éviter les faux négatifs)\n",
    "  * Crucial ici pour **ne pas manquer de vrais patients malades**\n",
    "\n",
    "* **Precision (précision)** : assurer que les cas prédits positifs sont réellement malades\n",
    "  * Évite les fausses alertes médicales\n",
    "\n",
    "**F1-score** (harmonique entre précision et rappel) est une bonne métrique synthétique ici.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Stratégies pour évaluer et améliorer le modèle**\n",
    "\n",
    "### Évaluation :\n",
    "\n",
    "* Utiliser **Recall, Precision, F1-score, ROC-AUC, PR-AUC**\n",
    "* Travailler avec une **matrice de confusion** pour voir les vrais résultats\n",
    "\n",
    "### Améliorations :\n",
    "\n",
    "1. **Rééchantillonnage des données** :\n",
    "\n",
    "   * **Oversampling** des cas positifs (ex : SMOTE)\n",
    "   * **Undersampling** des cas négatifs\n",
    "\n",
    "2. **Ajustement des poids de classes** dans les algorithmes\n",
    "   * Exemple : `class_weight=\"balanced\"` (Scikit-learn)\n",
    "\n",
    "3. **Seuil de décision ajusté**\n",
    "   * Baisser le seuil de prédiction pour augmenter le rappel\n",
    "\n",
    "4. **Modèles robustes** aux déséquilibres : XGBoost, LightGBM, etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87470a37",
   "metadata": {},
   "source": [
    "## Exercise 5 : Role of Threshold Tuning in Classification Models\n",
    "### Instructions\n",
    "You are evaluating a binary classification model that predicts whether a bank’s client will default on a loan. The model outputs a probability score between 0 and 1.\n",
    "- Describe how changing the threshold for classifying a positive case (default) from 0.5 to 0.7 might affect the model’s precision and recall.\n",
    "- Discuss the potential consequences of setting the threshold too high or too low in the context of loan default prediction.\n",
    "- Explain how ROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve) can assist in finding the optimal threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a4a72",
   "metadata": {},
   "source": [
    "## **Rôle du réglage du seuil dans les modèles de classification** (prédiction de défaut de prêt) :\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Effet d’un changement de seuil (ex : de 0.5 à 0.7)**\n",
    "\n",
    "* **Seuil = 0.5** : standard, au-dessus = défaut\n",
    "* **Seuil = 0.7** : il faut une probabilité plus élevée pour classer \"défaut\"\n",
    "\n",
    "### Conséquences :\n",
    "\n",
    "* **Précision augmente** (moins de faux positifs)\n",
    "* **Rappel diminue** (plus de vrais défauts sont manqués)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Risques selon le réglage du seuil**\n",
    "\n",
    "| Seuil trop **élevé** (ex : 0.9)                    | Seuil trop **bas** (ex : 0.2)                        |\n",
    "| -------------------------------------------------- | ---------------------------------------------------- |\n",
    "| * Peu de défauts détectés (faible **recall**)      | * Trop de clients classés \"à risque\" (**FP élevés**) |\n",
    "| * La banque prête à des gens qui vont faire défaut | * Trop de bons clients refusés ou pénalisés          |\n",
    "| * Risque financier élevé                           | * Perte de chiffre d’affaires ou d’image             |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Utilité de la courbe ROC et de l’AUC**\n",
    "\n",
    "* **Courbe ROC** : trace le **taux de rappel (TPR)** contre le **taux de faux positifs (FPR)** pour différents seuils.\n",
    "* **AUC (Area Under Curve)** : indique la capacité globale du modèle à distinguer les classes.\n",
    "\n",
    "### Utilité :\n",
    "\n",
    "* Aide à **choisir un seuil équilibré** selon l’objectif (maximiser le rappel, la précision ou le compromis).\n",
    "* Permet de **visualiser la performance** sans se fixer un seuil arbitraire à 0.5.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
