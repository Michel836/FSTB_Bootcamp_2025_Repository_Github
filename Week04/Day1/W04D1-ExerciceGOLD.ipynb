{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ca4e15",
   "metadata": {},
   "source": [
    "Exercises XP Gold\n",
    "Last Updated: January 8th, 2024\n",
    "\n",
    "What will you create\n",
    "calculation sheets for metrics like Accuracy, Precision, Recall, and F1-Score based on given confusion matrix values.\n",
    "develop strategic proposals that address specific challenges posed by these issues.\n",
    "build frameworks for evaluating classification models in different scenarios, especially focusing on the impact of class imbalance and the role of threshold tuning.\n",
    "\n",
    "\n",
    "What will you learn\n",
    "Understand and interpret the confusion matrix in different scenarios.\n",
    "Understand the trade-offs between different evaluation metrics.\n",
    "Explore the concepts of cross-validation and learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2a4ec",
   "metadata": {},
   "source": [
    "Exercise 1 : Analyzing Confusion Matrix\n",
    "Instructions\n",
    "Imagine you have a dataset for a binary classification problem, such as email spam detection, where emails are classified as either ‚ÄòSpam‚Äô or ‚ÄòNot Spam‚Äô. You are provided with the confusion matrix results of a classifier.\n",
    "- Define in your own words what True Positives, True Negatives, False Positives, and False Negatives mean in the context of this email spam detection problem.\n",
    "- Given a confusion matrix with specific values for TP, TN, FP, FN, calculate the Accuracy, Precision, Recall, and F1-Score.\n",
    "- Discuss how the classifier‚Äôs performance would change with a higher number of False Positives compared to False Negatives, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae06dd",
   "metadata": {},
   "source": [
    "Voici la r√©ponse compl√®te pour l‚Äô**Exercice 1 : Analyse d‚Äôune matrice de confusion dans un probl√®me de d√©tection de spam** :\n",
    "\n",
    "---\n",
    "\n",
    "## üì© **D√©finitions dans le contexte \"Spam / Pas Spam\"**\n",
    "\n",
    "* **True Positives (TP)** : Emails correctement identifi√©s comme **Spam**\n",
    "  ‚û§ Exemple : un vrai spam d√©tect√© comme tel.\n",
    "\n",
    "* **True Negatives (TN)** : Emails correctement identifi√©s comme **Non Spam**\n",
    "  ‚û§ Exemple : un email l√©gitime qui n‚Äôest pas marqu√© spam.\n",
    "\n",
    "* **False Positives (FP)** : Emails **non spam** mal class√©s comme **Spam**\n",
    "  ‚û§ Exemple : un email important mis en spam par erreur.\n",
    "\n",
    "* **False Negatives (FN)** : Emails **spam** mal class√©s comme **Non Spam**\n",
    "  ‚û§ Exemple : un spam qui atterrit dans la bo√Æte de r√©ception.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Calcul des m√©triques** (avec valeurs fictives)\n",
    "\n",
    "**Exemple** :\n",
    "\n",
    "* TP = 80\n",
    "* TN = 90\n",
    "* FP = 20\n",
    "* FN = 10\n",
    "\n",
    "Formules :\n",
    "\n",
    "* **Accuracy** = (TP + TN) / (TP + TN + FP + FN)\n",
    "  \\= (80 + 90) / (80 + 90 + 20 + 10) = 170 / 200 = **0.85**\n",
    "\n",
    "* **Precision** = TP / (TP + FP)\n",
    "  \\= 80 / (80 + 20) = **0.80**\n",
    "\n",
    "* **Recall** = TP / (TP + FN)\n",
    "  \\= 80 / (80 + 10) = **0.89**\n",
    "\n",
    "* **F1-Score** = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "  \\= 2 √ó (0.80 √ó 0.89) / (0.80 + 0.89) ‚âà **0.84**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è **Impact des erreurs FP vs FN**\n",
    "\n",
    "* **Trop de FP (faux positifs)** :\n",
    "  ‚û§ Trop d‚Äôemails **l√©gitimes** marqu√©s comme spam\n",
    "  ‚û§ Cela peut faire **perdre des messages importants** ‚Üí mauvaise exp√©rience utilisateur.\n",
    "\n",
    "* **Trop de FN (faux n√©gatifs)** :\n",
    "  ‚û§ Trop de **spams** non d√©tect√©s\n",
    "  ‚û§ Cela **pollue la bo√Æte de r√©ception**, mais moins grave si le filtre est globalement bon.\n",
    "\n",
    "üîé **Conclusion** :\n",
    "\n",
    "* Si l‚Äôenjeu est **ne pas rater des emails importants** ‚Üí il faut **minimiser les FP** (favoriser la **pr√©cision**).\n",
    "* Si l‚Äôenjeu est **filtrer tous les spams √† tout prix** ‚Üí il faut **minimiser les FN** (favoriser le **rappel**).\n",
    "\n",
    "Souhaites-tu un tableau ou document pour pr√©senter cela ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd3d7e",
   "metadata": {},
   "source": [
    "Exercise 2 : Evaluating Trade-offs in Metrics\n",
    "Instructions\n",
    "Consider a medical diagnosis application where a model predicts whether patients have a certain disease.\n",
    "- Explain why high recall is more important than high precision in this medical diagnosis context.\n",
    "- Describe a scenario where precision becomes more important than recall.\n",
    "- Discuss the potential consequences of focusing solely on improving accuracy in imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b789f3",
   "metadata": {},
   "source": [
    "Voici la r√©ponse compl√®te pour l‚Äô**Exercice 2 : √âvaluer les compromis entre les m√©triques dans un contexte m√©dical** :\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **1. Pourquoi le rappel (Recall) est plus important que la pr√©cision dans ce cas m√©dical ?**\n",
    "\n",
    "* **Rappel √©lev√©** = d√©tecter **le plus grand nombre possible de malades**\n",
    "* Dans un contexte m√©dical, **rater un patient malade (FN)** peut entra√Æner un **retard de diagnostic** ou **mettre sa vie en danger**.\n",
    "* Il vaut mieux **faux alarmer** un patient sain (FP) que **louper un vrai cas**.\n",
    "\n",
    "üëâ **Mieux vaut alerter √† tort qu‚Äôignorer un malade.**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **2. Quand la pr√©cision devient-elle plus importante ?**\n",
    "\n",
    "**Exemple** : Une alerte automatique pour d√©clencher un traitement co√ªteux, invasif ou risqu√© (ex : chimioth√©rapie, chirurgie).\n",
    "\n",
    "* **Pr√©cision √©lev√©e** = on veut **√©viter les faux positifs**\n",
    "* On veut √™tre s√ªr que **les cas d√©tect√©s sont r√©ellement positifs** avant d‚Äôagir lourdement.\n",
    "\n",
    "üëâ **Mieux vaut ne rien faire que de faire du mal inutilement.**\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ **3. Probl√®mes si on optimise uniquement l‚ÄôAccuracy sur un jeu d√©s√©quilibr√©**\n",
    "\n",
    "* Si 99 % des patients sont sains, un mod√®le qui pr√©dit \"sain\" pour tout le monde aura **99 % de pr√©cision**... mais il **rate tous les malades** !\n",
    "* L‚Äôaccuracy **masque les erreurs critiques** dans des cas d√©s√©quilibr√©s.\n",
    "\n",
    "üî¥ **Cons√©quence** : mod√®le apparemment performant, mais **inutilisable en pratique**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Il est donc **essentiel de choisir la bonne m√©trique** selon le **contexte, le risque et les cons√©quences** des erreurs. Souhaites-tu un r√©sum√© visuel ou document de cette analyse ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5d03f",
   "metadata": {},
   "source": [
    "Exercise 3 : Understanding Cross-Validation and Learning Curves\n",
    "Instructions\n",
    "You are working on a project with a large dataset that involves predicting housing prices based on various features.\n",
    "- Explain the difference between K-Fold Cross-Validation and Stratified K-Fold Cross-Validation. Which one would you choose for this task and why?\n",
    "- Describe what learning curves are and how they can help in understanding the performance of your model.\n",
    "- Discuss the implications of underfitting and overfitting as observed from learning curves, and how you might address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd24473",
   "metadata": {},
   "source": [
    "Voici la r√©ponse compl√®te pour l‚Äô**Exercice 3 : Validation crois√©e et courbes d‚Äôapprentissage (projet de pr√©diction de prix immobiliers)** :\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ **1. Diff√©rence entre K-Fold et Stratified K-Fold**\n",
    "\n",
    "### **K-Fold Cross-Validation**\n",
    "\n",
    "* Divise les donn√©es en *k* sous-ensembles.\n",
    "* √Ä chaque it√©ration, un pli est utilis√© pour tester, les autres pour entra√Æner.\n",
    "* Utilis√© pour √©valuer la stabilit√© d‚Äôun mod√®le.\n",
    "\n",
    "### **Stratified K-Fold Cross-Validation**\n",
    "\n",
    "* M√™me principe, **mais en conservant la proportion des classes** (utile pour classification).\n",
    "* Assure un bon √©quilibre entre classes dans chaque pli.\n",
    "\n",
    "‚û°Ô∏è **Choix ici** : **K-Fold classique**, car **la r√©gression (prix)** ne n√©cessite pas de balance entre classes.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **2. Courbes d‚Äôapprentissage (Learning Curves)**\n",
    "\n",
    "Les **courbes d‚Äôapprentissage** montrent l‚Äô√©volution des performances du mod√®le en fonction de la taille de l‚Äô√©chantillon d‚Äôentra√Ænement.\n",
    "\n",
    "* Deux courbes : **erreur d‚Äôentra√Ænement** et **erreur de validation**\n",
    "* Permettent de diagnostiquer :\n",
    "\n",
    "  * Si le mod√®le apprend correctement\n",
    "  * Si plus de donn√©es am√©liorent la performance\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **3. Underfitting vs Overfitting selon les courbes**\n",
    "\n",
    "| Situation        | Sympt√¥mes sur les courbes                      | Solutions possibles                                             |\n",
    "| ---------------- | ---------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **Underfitting** | Erreurs √©lev√©es sur train **et** validation    | ‚û§ Mod√®le trop simple<br>‚û§ Ajouter features ou complexit√©        |\n",
    "| **Overfitting**  | Erreur faible sur train, √©lev√©e sur validation | ‚û§ Mod√®le trop complexe<br>‚û§ Regularisation<br>‚û§ Plus de donn√©es |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Les courbes d‚Äôapprentissage aident √† **ajuster la complexit√© du mod√®le** et **guider les am√©liorations**.\n",
    "\n",
    "Souhaites-tu un sch√©ma ou fichier Word avec tout cela ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65284a10",
   "metadata": {},
   "source": [
    "Exercise 4 : Impact of Class Imbalance on Model Evaluation\n",
    "Instructions\n",
    "Imagine you are working on a dataset for detecting a rare disease where only 2% of the instances are positive cases (have the disease).\n",
    "- Explain why using accuracy as an evaluation metric might be misleading in this scenario.\n",
    "- Discuss the importance of precision and recall in the context of this imbalanced dataset.\n",
    "- Propose strategies you could use to more effectively evaluate and improve the model‚Äôs performance in this scenario, considering the imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e5f59",
   "metadata": {},
   "source": [
    "Voici la r√©ponse compl√®te pour l‚Äô**Exercice 4 : Impact du d√©s√©quilibre des classes sur l‚Äô√©valuation du mod√®le** (d√©tection d‚Äôune maladie rare √† 2 %) :\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå **1. Pourquoi l‚Äôaccuracy est trompeuse ici**\n",
    "\n",
    "* Si 98 % des cas sont n√©gatifs (sains), un mod√®le qui pr√©dit \"n√©gatif\" pour tous aura **98 % d‚Äôaccuracy**‚Ä¶\n",
    "  ‚û°Ô∏è **Mais il ne d√©tecte aucun malade.**\n",
    "\n",
    "üî¥ **Accuracy** donne donc une **illusion de performance** dans les jeux de donn√©es d√©s√©quilibr√©s.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **2. Importance de la pr√©cision et du rappel**\n",
    "\n",
    "* **Recall (rappel)** : d√©tecter un maximum de cas malades (√©viter les faux n√©gatifs)\n",
    "  ‚û§ Crucial ici pour **ne pas manquer de vrais patients malades**\n",
    "\n",
    "* **Precision (pr√©cision)** : assurer que les cas pr√©dits positifs sont r√©ellement malades\n",
    "  ‚û§ √âvite les fausses alertes m√©dicales\n",
    "\n",
    "‚úÖ **F1-score** (harmonique entre pr√©cision et rappel) est une bonne m√©trique synth√©tique ici.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **3. Strat√©gies pour √©valuer et am√©liorer le mod√®le**\n",
    "\n",
    "### ‚úÖ √âvaluation :\n",
    "\n",
    "* Utiliser **Recall, Precision, F1-score, ROC-AUC, PR-AUC**\n",
    "* Travailler avec une **matrice de confusion** pour voir les vrais r√©sultats\n",
    "\n",
    "### ‚öôÔ∏è Am√©liorations :\n",
    "\n",
    "1. **R√©√©chantillonnage des donn√©es** :\n",
    "\n",
    "   * **Oversampling** des cas positifs (ex : SMOTE)\n",
    "   * **Undersampling** des cas n√©gatifs\n",
    "\n",
    "2. **Ajustement des poids de classes** dans les algorithmes\n",
    "   ‚û§ Exemple : `class_weight=\"balanced\"` (Scikit-learn)\n",
    "\n",
    "3. **Seuil de d√©cision ajust√©**\n",
    "   ‚û§ Baisser le seuil de pr√©diction pour augmenter le rappel\n",
    "\n",
    "4. **Mod√®les robustes** aux d√©s√©quilibres : XGBoost, LightGBM, etc.\n",
    "\n",
    "---\n",
    "\n",
    "Souhaites-tu un document rassemblant tous les exercices sur l'√©valuation des mod√®les ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87470a37",
   "metadata": {},
   "source": [
    "Exercise 5 : Role of Threshold Tuning in Classification Models\n",
    "Instructions\n",
    "You are evaluating a binary classification model that predicts whether a bank‚Äôs client will default on a loan. The model outputs a probability score between 0 and 1.\n",
    "- Describe how changing the threshold for classifying a positive case (default) from 0.5 to 0.7 might affect the model‚Äôs precision and recall.\n",
    "- Discuss the potential consequences of setting the threshold too high or too low in the context of loan default prediction.\n",
    "- Explain how ROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve) can assist in finding the optimal threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a4a72",
   "metadata": {},
   "source": [
    "Voici la r√©ponse compl√®te pour l‚Äô**Exercice 5 : R√¥le du r√©glage du seuil dans les mod√®les de classification** (pr√©diction de d√©faut de pr√™t) :\n",
    "\n",
    "---\n",
    "\n",
    "## üéöÔ∏è **1. Effet d‚Äôun changement de seuil (ex : de 0.5 √† 0.7)**\n",
    "\n",
    "* **Seuil = 0.5** : standard, au-dessus = d√©faut\n",
    "* **Seuil = 0.7** : il faut une probabilit√© plus √©lev√©e pour classer \"d√©faut\"\n",
    "\n",
    "### Cons√©quences :\n",
    "\n",
    "* ‚ûï **Pr√©cision augmente** (moins de faux positifs)\n",
    "* ‚ûñ **Rappel diminue** (plus de vrais d√©fauts sont manqu√©s)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **2. Risques selon le r√©glage du seuil**\n",
    "\n",
    "| Seuil trop **√©lev√©** (ex : 0.9)                    | Seuil trop **bas** (ex : 0.2)                        |\n",
    "| -------------------------------------------------- | ---------------------------------------------------- |\n",
    "| ‚û§ Peu de d√©fauts d√©tect√©s (faible **recall**)      | ‚û§ Trop de clients class√©s \"√† risque\" (**FP √©lev√©s**) |\n",
    "| ‚û§ La banque pr√™te √† des gens qui vont faire d√©faut | ‚û§ Trop de bons clients refus√©s ou p√©nalis√©s          |\n",
    "| ‚û§ Risque financier √©lev√©                           | ‚û§ Perte de chiffre d‚Äôaffaires ou d‚Äôimage             |\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **3. Utilit√© de la courbe ROC et de l‚ÄôAUC**\n",
    "\n",
    "* **Courbe ROC** : trace le **taux de rappel (TPR)** contre le **taux de faux positifs (FPR)** pour diff√©rents seuils.\n",
    "* **AUC (Area Under Curve)** : indique la capacit√© globale du mod√®le √† distinguer les classes.\n",
    "\n",
    "### ‚û§ Utilit√© :\n",
    "\n",
    "* Aide √† **choisir un seuil √©quilibr√©** selon l‚Äôobjectif (maximiser le rappel, la pr√©cision ou le compromis).\n",
    "* Permet de **visualiser la performance** sans se fixer un seuil arbitraire √† 0.5.\n",
    "\n",
    "---\n",
    "\n",
    "Souhaites-tu une synth√®se graphique ou un document avec les 5 exercices sur **le r√©glage des seuils et l‚Äô√©valuation** ?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
